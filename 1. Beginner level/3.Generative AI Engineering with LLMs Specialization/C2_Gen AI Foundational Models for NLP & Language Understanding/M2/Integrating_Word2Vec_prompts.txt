1.Write a Python function that takes word embeddings and their vocabulary as input and uses t-SNE to reduce the embeddings to two dimensions. Plot the embeddings on a 2D scatter plot and label each point with the corresponding word from the vocabulary. The function should use matplotlib for plotting and annotate each point with the word. Assume the vocabulary can be accessed using vocab.get_itos().

2.Write a function in Python that takes a word and a set of word embeddings as input. It should calculate the similarity between the word and other words in the embeddings using cosine similarity and return a list of the top k similar words. If the word is not found, it should return an empty list and print a message.

3.Write a Python function called train_model that takes a PyTorch model, a DataLoader for training data, a loss function, an optimizer, and the number of epochs as inputs. The function should train the model and return the trained model along with a list of average losses for each epoch.

toy_data ="..................."
4.Generate a Python function that takes a long string of sentences, tokenizes them into words using a basic English tokenizer, and creates a vocabulary from the tokens while handling unknown words with a special token.

5.Write Python code that, given a tokenized version of a text (such as toy_data), loops through the tokens and creates context-target pairs. The context should consist of two words before and two words after each target word. Print two of the context-target pairs at the end.

6.Write Python code that takes a batch of context-target word pairs (like cobow_data), converts the target and context words into their numeric form using a vocabulary, and organizes the results into three tensors: one for target words, one for context words, and one for offsets that mark where each context starts. The function should return these tensors, and a small batch should be tested and printed.

7.Write Python code that uses a DataLoader to divide the cobow_data into batches of 64, shuffle the data, and use the collate_batch function to prepare each batch. Then, print the dataloader to see the details.

8.Write Python code to create a Continuous Bag of Words (CBOW) model using PyTorch. The model should have an embedding layer, a linear layer that halves the embedding size, and a final fully connected layer to predict the word from the context. Initialize the weights and biases. Then, create a CBOW model with an embedding size of 24 and move it to the device.

9.Write Python code to set up the training process for a CBOW model using PyTorch. Include the following components:
A learning rate (LR) of 5,
A loss function using CrossEntropyLoss,
An optimizer using stochastic gradient descent (SGD) to update the model parameters,
A learning rate scheduler that reduces the learning rate by a factor of 0.1 every epoch,
Train the model for 400 epochs, and plot the loss over epochs.

10.Write Python code to extract and visualize word embeddings from a CBOW model in PyTorch. Include the following steps:
Get the weight of the embedding layer and convert it into a NumPy array.
Specify a word (e.g., 'baller') and find its index in the vocabulary.
Print the embeddings for the specified word.
Create a plot to visualize the word embeddings for all words in the vocabulary.

11.Write Python code to generate (target, context) word pairs using a context window size of 2. For each word in a tokenized dataset, collect 2 words before and 2 words after it as context, and then flatten the resulting list to create individual (target, context) pairs.

12.Write Python code to create a collate function that processes pairs of (context, target) words from a list. Convert the context and target words to their corresponding numerical IDs using a vocab dictionary, and then return them as tensors. Create a DataLoader to return batches of these tensors.

13.Write a PyTorch class for a Skip-Gram model with an embeddings layer that maps words to embeddings and a fully connected layer that predicts context words from the embeddings. Define the forward pass that applies ReLU to the embeddings before passing them to the fully connected layer. Initialize the model with a vocabulary size and embedding dimension.

14.Write PyTorch code to train a Skip-Gram model using stochastic gradient descent (SGD) and CrossEntropyLoss. The model should be trained using a learning rate of 5 and a batch size of 64. After training, plot the loss over epochs using matplotlib. The train_model() function is predefined, which handles the training loop. Once training is complete, extract the word embeddings from the model and plot them using a function plot_embeddings() that accepts the embeddings and vocabulary.


Applying pretrained word embeddings:
1.Write PyTorch code that:
Loads the pre-trained GloVe word embeddings (specifically the 6B version).
Converts these embeddings into a PyTorch embedding layer.
Maps words to their corresponding index from the GloVe vocabulary and shows how to retrieve the vector representation of the word 'team' from the embedding layer.

2.Write PyTorch code that:
Takes an array of example words.
Retrieves the corresponding GloVe embeddings for each word using the GloVe model.
Stores these embeddings in a dictionary.
Finds and prints the top 2 most similar words to a target word ('small') based on cosine similarity, using a predefined function find_similar_words.

Train a word2vec model from gensim:
1.Write Python code using Gensim's Word2Vec to:
Define a list of sentences, each containing a list of words.
Lowercase all words in the sentences.
Create a Word2Vec model with specified parameters (e.g., vector size, window size).
Build the vocabulary and train the model.
Find similar words to a target word ('pizza') and calculate the similarity between two words ('pizza' and 'pasta').
Extract the word vectors and create a word-to-index mapping.
Create a PyTorch embedding layer initialized with the trained Word2Vec vectors and demonstrate how to get the embedding for a specific word.

Text classification using pretrained word embeddings:
Write a Python script to implement a text classification model using PyTorch and TorchText. The model should use GloVe embeddings to represent words and should be trained on the AG_NEWS dataset. Include the following features:
1. Load GloVe embeddings and create a vocabulary with special tokens for unknown and padding.
2. Define a tokenizer for basic English.
3. Load and split the AG_NEWS dataset into training and validation sets.
4. Create functions to process text and labels.
5. Implement a collate function to create batches for training.
6. Define a PyTorch model for text classification that uses the GloVe embeddings.
7. Create functions for evaluating the model's performance.
8. Implement a training loop with hyperparameter settings and save the best model.
9. Plot the training loss and accuracy over epochs.
10. Evaluate the final model on the test dataset.






