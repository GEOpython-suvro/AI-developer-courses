<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="renderer" content="webkit" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" />
    <meta name="keywords" content="Editor.md,editor,Markdown Editor,Markdown" />
    <meta name="description" content="Skills Network Author IDE" />
    <title>Skills Network Editor</title>
    <link rel="stylesheet" href="/editormd/examples/css/style.css?version=3.3.5" />
    <link rel="stylesheet" href="/editormd/css/editormd.css?version=3.3.5" />
    <link rel="stylesheet" href="/public/css/custom.css?version=3.3.5" />
    <link rel="stylesheet" href="/public/css/custom_popover.css?version=3.3.5" />
    <link rel="shortcut icon" href="/public/images/SN_favicon.png" type="image/x-icon" />

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"
        integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
    <script src="/public/js/comms.js?version=3.3.5"></script>
    <script src="//unpkg.com/alpinejs"></script>
</head>

<body>
    <div id="layout">
    <div id="test-editormd">
        <textarea style="display:none;">## Cheat Sheet: Generative AI Overview and Data Preparation

&lt;table&gt;
&lt;colgroup&gt;
&lt;col style&#x3D;&quot;width: 9%&quot; /&gt;
&lt;col style&#x3D;&quot;width: 57%&quot; /&gt;
&lt;col style&#x3D;&quot;width: 57%&quot; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class&#x3D;&quot;header&quot;&gt;
&lt;th&gt;Package/Method&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Code example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class&#x3D;&quot;odd&quot;&gt;
&lt;td&gt;NLTK&lt;/td&gt;
&lt;td&gt;NLTK is a Python library used in natural language processing (NLP) for tasks such as tokenization and text processing.
The code example shows how you can tokenize text using the NLTK word-based tokenizer.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;import nltk
nltk.download(&quot;punkt&quot;)
from nltk.tokenize import word_tokenize
text &#x3D; &quot;Unicorns are real. I saw a unicorn yesterday. I couldn&#x27;t see it today.&quot;
token &#x3D; word_tokenize(text)
print(token)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;even&quot;&gt;
&lt;td&gt;spaCy&lt;/td&gt;
&lt;td&gt;spaCy is an open-source library used in NLP. It provides tools for tasks such as tokenization and word embeddings.
The code example shows how you can tokenize text using spaCy word-based tokenizer.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;import spacy
text &#x3D; &quot;Unicorns are real. I saw a unicorn yesterday. I couldn&#x27;t see it today.&quot;
nlp &#x3D; spacy.load(&quot;en_core_web_sm&quot;)
doc &#x3D; nlp(text)
token_list &#x3D; [token.text for token in doc]
print(&quot;Tokens:&quot;, token_list)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;odd&quot;&gt;
&lt;td&gt;BertTokenizer&lt;/td&gt;
&lt;td&gt;BertTokenizer is a subword-based tokenizer that uses the WordPiece algorithm.
The code example shows how you can tokenize text using BertTokenizer.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;from transformers import BertTokenizer
tokenizer &#x3D; BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
tokenizer.tokenize(&quot;IBM taught me tokenization.&quot;)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;even&quot;&gt;
&lt;td&gt;XLNetTokenizer&lt;/td&gt;
&lt;td&gt;XLNetTokenizer tokenizes text using Unigram and SentencePiece algorithms.
The code example shows how you can tokenize text using XLNetTokenizer.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;from transformers import XLNetTokenizer
tokenizer &#x3D; XLNetTokenizer.from_pretrained(&quot;xlnet-base-cased&quot;)
tokenizer.tokenize(&quot;IBM taught me tokenization.&quot;)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;odd&quot;&gt;
&lt;td&gt;torchtext&lt;/td&gt;
&lt;td&gt;The torchtext library is part of the PyTorch ecosystem and provides the tools and functionalities required for NLP.
The code example shows how you can use torchtext to generate tokens and convert them to indices.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;from torchtext.vocab import build_vocab_from_iterator
# Defines a dataset
dataset &#x3D; [
	(1,&quot;Introduction to NLP&quot;),
	(2,&quot;Basics of PyTorch&quot;),
	(1,&quot;NLP Techniques for Text Classification&quot;),
	(3,&quot;Named Entity Recognition with PyTorch&quot;),
	(3,&quot;Sentiment Analysis using PyTorch&quot;),
	(3,&quot;Machine Translation with PyTorch&quot;),
	(1,&quot;NLP Named Entity,Sentiment Analysis, Machine Translation&quot;),
	(1,&quot;Machine Translation with NLP&quot;),
	(1,&quot;Named Entity vs Sentiment Analysis NLP&quot;)]
# Applies the tokenizer to the text to get the tokens as a list
from torchtext.data.utils import get_tokenizer
tokenizer &#x3D; get_tokenizer(&quot;basic_english&quot;)
tokenizer(dataset[0][1])
# Takes a data iterator as input, processes text from the iterator, 
# and yields the tokenized output individually
def yield_tokens(data_iter):
	for _,text in data_iter:
		yield tokenizer(text)
# Creates an iterator
my_iterator &#x3D; yield_tokens(dataset)
# Fetches the next set of tokens from the data set
next(my_iterator)
# Converts tokens to indices and sets &amp;lt;unk&amp;gt; as the 
# default word if a word is not found in the vocabulary
vocab &#x3D; build_vocab_from_iterator(yield_tokens(dataset), specials&#x3D;[&quot;&amp;lt;unk&amp;gt;&quot;])
vocab.set_default_index(vocab[&quot;&amp;lt;unk&amp;gt;&quot;])
# Gives a dictionary that maps words to their corresponding numerical indices
vocab.get_stoi()&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;even&quot;&gt;
&lt;td&gt;vocab&lt;/td&gt;
&lt;td&gt;The vocab object is part of the PyTorch torchtext library. It maps tokens to indices.
The code example shows how you can apply the vocab object to tokens directly.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;# Takes an iterator as input and extracts the next tokenized sentence. 
# Creates a list of token indices using the vocab dictionary for each token.
def get_tokenized_sentence_and_indices(iterator):
	tokenized_sentence &#x3D; next(iterator)
	token_indices &#x3D; [vocab[token] for token in tokenized_sentence]
	return tokenized_sentence, token_indices
# Returns the tokenized sentences and the corresponding token indices. 
# Repeats the process.
tokenized_sentence, token_indices &#x3D; \
get_tokenized_sentence_and_indices(my_iterator)
next(my_iterator)
# Prints the tokenized sentence and its corresponding token indices.
print(&quot;Tokenized Sentence:&quot;, tokenized_sentence)
print(&quot;Token Indices:&quot;, token_indices)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;odd&quot;&gt;
&lt;td&gt;Special tokens in PyTorch:
&amp;lt;eos&amp;gt; and &amp;lt;bos&amp;gt;&lt;/td&gt;
&lt;td&gt;Special tokens are tokens introduced to input sequences to convey specific information or serve a particular purpose during training.
The code example shows the use of &amp;lt;bos&amp;gt; and &amp;lt;eos&amp;gt; during
tokenization. The &amp;lt;bos&amp;gt; token denotes the beginning of the input
sequence, and the &amp;lt;eos&amp;gt; token denotes the end.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;# Appends &amp;lt;bos&amp;gt; at the beginning and &amp;lt;eos&amp;gt; at the end of the tokenized sentences 
# using a loop that iterates over the sentences in the input data
tokenizer_en &#x3D; get_tokenizer(&#x27;spacy&#x27;, language&#x3D;&#x27;en_core_web_sm&#x27;)
tokens &#x3D; []
max_length &#x3D; 0
for line in lines:
	tokenized_line &#x3D; tokenizer_en(line)
	tokenized_line &#x3D; [&#x27;&amp;lt;bos&amp;gt;&#x27;] + tokenized_line + [&#x27;&amp;lt;eos&amp;gt;&#x27;]
	tokens.append(tokenized_line)
	max_length &#x3D; max(max_length, len(tokenized_line))&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;even&quot;&gt;
&lt;td&gt;Special tokens in PyTorch:
&amp;lt;pad&amp;gt;&lt;/td&gt;
&lt;td&gt;The code example shows the use of &amp;lt;pad&amp;gt; token to ensure all
sentences have the same length.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;# Pads the tokenized lines
for i in range(len(tokens)):
	tokens[i] &#x3D; tokens[i] + [&#x27;&amp;lt;pad&amp;gt;&#x27;] * (max_length - len(tokens[i]))&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;odd&quot;&gt;
&lt;td&gt;Dataset class in
PyTorch&lt;/td&gt;
&lt;td&gt;The Dataset class enables accessing and retrieving individual samples from a data set.
The code example shows how you can create a custom data set and access samples.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;# Imports the Dataset class and defines a list of sentences
from torch.utils.data import Dataset
sentences &#x3D; [&quot;If you want to know what a man&#x27;s like, take a 
good look at how he treats his inferiors, not his equals.&quot;, 
&quot;Fae&#x27;s a fickle friend, Harry.&quot;]
# Downloads and reads data
class CustomDataset(Dataset):
	def __init__(self, sentences):
		self.sentences &#x3D; sentences
	# Returns the data length
	def __len__(self):
		return len(self.sentences)
	# Returns one item on the index
	def __getitem__(self, idx):
		return self.sentences[idx]
# Creates a dataset object
dataset&#x3D;CustomDataset(sentences)
# Accesses samples like in a list
E.g., dataset[0]&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;even&quot;&gt;
&lt;td&gt;DataLoader class in PyTorch&lt;/td&gt;
&lt;td&gt;A DataLoader class enables efficient loading and iteration over data sets for training deep learning models.
The code example shows how you can use the DataLoader class to generate batches of sentences for further processing, such as training a neural network model&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;# Creates an iterator object
data_iter &#x3D; iter(dataloader)
# Calls the next function to return new batches of samples
next(data_iter)
# Creates an instance of the custom data set
from torch.utils.data import DataLoader
custom_dataset &#x3D; CustomDataset(sentences)
# Specifies a batch size
batch_size &#x3D; 2
# Creates a data loader
dataloader &#x3D; DataLoader(custom_dataset, batch_size&#x3D;batch_size, shuffle&#x3D;True)
# Prints the sentences in each batch&lt;br /&gt;
for batch in dataloader:
	print(batch)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class&#x3D;&quot;odd&quot;&gt;
&lt;td&gt;Custom collate function in PyTorch&lt;/td&gt;
&lt;td&gt;The custom collate function is a user-defined function that defines how individual samples are collated or batched together. You can utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor.
The code example shows how you can use a custom collate function in a
data loader.&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;# Defines a custom collate function
def collate_fn(batch):
	tensor_batch &#x3D; []
# Tokenizes each sample in the batch
	for sample in batch:
		tokens &#x3D; tokenizer(sample)
# Maps tokens to numbers using the vocab
		tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))
# Pads the sequences within the batch to have equal lengths
	padded_batch &#x3D; pad_sequence(tensor_batch,batch_first&#x3D;True)
	return padded_batch
# Creates a data loader using the collate function and the custom dataset
dataloader &#x3D; DataLoader(custom_dataset, batch_size&#x3D;batch_size, shuffle&#x3D;True, collate_fn&#x3D;collate_fn)&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/footer_logo_sn_ibm.png)</textarea>
    </div>
</div>
<script src="https://cdn.jsdelivr.net/npm/he@1.2.0/he.min.js"></script>
<script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/iframe-resizer/4.3.6/iframeResizer.contentWindow.min.js"></script>

<script src="https://cdn.plyr.io/3.7.8/plyr.polyfilled.js"></script>
<link rel="stylesheet" href="https://cdn.plyr.io/3.7.8/plyr.css" />
<link rel="stylesheet" href="public/css/custom_plyr.css?version=3.3.5" />

<script src="editormd/examples/js/jquery.min.js"></script>
<script src="editormd/editormd.js?version=3.3.5"></script>

<script src="editormd/languages/en.js"></script>

<script src="public/js/common.js?version=3.3.5"></script>
<script src="public/js/render.js?version=3.3.5"></script>

<script type="text/javascript">


    var authorEditor = $(function () {
        let metadata = he.decode('{&quot;markdown-version&quot;:&quot;v1&quot;,&quot;tool-type&quot;:&quot;instructional-lab&quot;,&quot;branch&quot;:&quot;lab-9545-instruction&quot;,&quot;version-history-start-date&quot;:&quot;2024-01-19T11:28:18Z&quot;,&quot;audio-timestamp-of-request-to-generate-wav&quot;:&quot;2024-02-22T16:44:26Z&quot;,&quot;audio-file-url&quot;:&quot;https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/labs/Cheat_sheet/C1/Generative_AI_Overview_and_Data_Preparation.md.wav&quot;}')
        injectMetadata(metadata)

        let token = findGetParameter("token");

        let LABS_BASE_URL = 'https://labs.cognitiveclass.ai';
        let asset_library_prefix_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera';
        let tool_type = "instructional-lab";
        let filename = `labs/Cheat_sheet/C1/Generative_AI_Overview_and_Data_Preparation.md`;
        let audioFileUrl = `https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/labs/Cheat_sheet/C1/Generative_AI_Overview_and_Data_Preparation.md.wav`
        let currentTheme = localStorage.getItem("theme") || (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? "dark" : "default");

        return authorEditor = editormd("test-editormd", "3.3.5", {
            tool_type,
            filename,
            audioFileUrl,
            width: "100%",
            //height: "100%",
            autoHeight: true,
            // mode: 'markdown',
            theme: currentTheme,
            previewTheme: currentTheme,
            editorTheme: currentTheme === "dark" ? "pastel-on-dark" : "default",
            readOnly: true,
            pluginPath: 'plugins/',
            syncScrolling: false,
            htmlDecode: 'img',
            path: 'editormd/lib/',
            watch: false,
            // markdown,
            toolbar: false,
            atLink: false,
            toc: true,
            tocm: true,
            tocDropdown: false,
            tocTitle: "Table of Contents For your lab!",
            tex: true,
            flowChart: true,
            sequenceDiagram: true,
            codeFold: true, // Doesn't do anything?
            taskList: true,
            emoji: true,
            saveMarkdownToTextarea: true,

            onresize: function () {
                $("html,body").css("overflow", "hidden");

                this.preview.css({
                    width: $(window).width(),
                    height: $(window).height()
                })

                adjustProgressBarWidth()
            },
            fixCodeBlocks: function () {
                adjustCodeBlocks(this);
            },
            onload: function () {
                $("[type=\"file\"]").bind("change", function () {
                    alert($(this).val());
                    authorEditor.cm.replaceSelection($(this).val());
                    console.log($(this).val(), authorEditor);
                });

                this.fullscreen();
                this.previewing();

                setPreviewWatchToolbar(this);

                if (tool_type != "instructional-lab") {
                    fixCustomPlugins(this, 'https://reward.skills.network/claim');
                }
                else {
                    if(audioFileUrl) {
                        addAudioWidget(this, audioFileUrl);
                    }
                }

                setAssetLibraryPrefixUrl(asset_library_prefix_url);
                //comms with UI
                setParentUrl(LABS_BASE_URL);
                establishCommsWithUI(this);
                //let the UI know that the author-ide has loaded
                requestToUI({ type: "frame_loaded" });
            }
        });
    });
</script>
</body>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns" crossorigin="anonymous">
    </script>

</html>