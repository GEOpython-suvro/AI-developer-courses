{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "%pip install Levenshtein\n",
    "%pip install matplotlib\n",
    "%pip install torch==2.3.0 torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "from Levenshtein import distance\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'train'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000              # Maximum training iterations\n",
    "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
    "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
    "\n",
    "# Architecture parameters\n",
    "max_vocab_size = 256          # Maximum vocabulary size\n",
    "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
    "block_size = 16               # Context length for predictions\n",
    "n_embd = 32                   # Embedding size\n",
    "num_heads = 2                 # Number of head in multi-headed attention\n",
    "n_layer = 2                   # Number of Blocks\n",
    "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
    "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
    "\n",
    "head_size = n_embd // num_heads\n",
    "assert (num_heads * head_size) == n_embd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the parameter setup, you will create a function defined as `plot_embeddings`, which is designed to visualize the learned embeddings in a 3D space using `matplotlib`. This helps in understanding how the embeddings cluster and separate different tokens, providing insight into what the model has learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define 'vocabularies'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
      "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
     ]
    }
   ],
   "source": [
    "# Create and sort the input vocabulary from the dictionary's keys\n",
    "vocabulary_in = torch.sorted(list(set(dictionary.keys())))\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(vocabulary_in)\n",
    "\n",
    "# Create and sort the output vocabulary from the dictionary's values\n",
    "vocabulary_out = torch.sorted(list(set(dictionary.values())))\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(vocabulary_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode tokens using 'one hot' encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet focuses on creating and one-hot encoding vocabularies for both the input and output languages based on a predefined dictionary. The process begins with extracting the keys and values from the dictionary, resulting in two distinct vocabularies: one for the source language (French) and one for the target language (English).\n",
    "\n",
    "First, the unique words from the input dictionary are sorted and stored in vocabulary_in, and its size is printed. Similarly, the output vocabulary, vocabulary_out, is created from the dictionary values and displayed.\n",
    "\n",
    "The core functionality lies in the encode_one_hot function, which transforms the vocabulary into one-hot encoded vectors. For a vocabulary of size \n",
    "\n",
    "N, each word is represented as a vector of length \n",
    "\n",
    "N where only one element is set to 1 (indicating the presence of that word) and all other elements are 0. Mathematically, the one-hot encoding for a word \n",
    "ùë§\n",
    "ùëñ\n",
    "w \n",
    "i\n",
    "‚Äã\n",
    "  in a vocabulary of size \n",
    "\n",
    "N can be expressed as:\n",
    "\n",
    "ùê∏\n",
    "(\n",
    "ùë§\n",
    "ùëñ\n",
    ")\n",
    "\n",
    "\n",
    "During the encoding process, a zero vector is initialized for each word in the vocabulary. The vector is then updated by setting the position corresponding to the word‚Äôs index to 1. Each word and its corresponding one-hot encoded vector are printed for visualization, providing insight into the representation.\n",
    "\n",
    "The one-hot encoding is applied to both the input vocabulary and the output vocabulary, resulting in two dictionaries: one_hot_in and one_hot_out. This representation is crucial for feeding the encoded inputs into neural network models, as it allows the model to process textual data in a format suitable for mathematical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
    "def encode_one_hot(vocabulary):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return   # Return the dictionary of words and their one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
      "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
      "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
      "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
      "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
      "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
    "one_hot_in = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
      "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
      "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
      "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
      "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
      "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
    "# This visualizes the one-hot representation for each word in the input vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\t: tensor([1., 0., 0., 0., 0.])\n",
      "is\t: tensor([0., 1., 0., 0., 0.])\n",
      "table\t: tensor([0., 0., 1., 0., 0.])\n",
      "the\t: tensor([0., 0., 0., 1., 0.])\n",
      "under\t: tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
    "# This time we're encoding the target language vocabulary\n",
    "one_hot_out = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a 'dictionary' using matrix multiplication\n",
    "\n",
    "We're now illustrating how to create a representation of our dictionary suitable for neural network operations:\n",
    "\n",
    "- **Matrix creation**: Using PyTorch's `torch.stack`, convert the one-hot encoded vectors for both input (`K`) and output (`V`) vocabularies into tensors. `K` is constructed from the input vocabulary's one-hot vectors, and `V` from the output vocabulary's vectors. These tensors can be thought of as a look-up table that our model will use to associate input tokens with output tokens.\n",
    "\n",
    "- **Dictionary as matrices**: This step effectively translates our word-to-word dictionary mapping into a neural network-friendly format. Each row in `K` corresponds to a word in the input language represented as a one-hot vector, and each row in `V` corresponds to the respective translated word in the output language.\n",
    "\n",
    "- **Query example**: An example shows how to use matrix operations to find a translation. Look up the one-hot vector for the word \"sous\" from the input vocabulary (`q`). Then demonstrate how to find its corresponding translation by performing matrix multiplication with the transpose of `K` (i.e., `q @ K.T`) to identify the index and then use that index to select the relevant row from `V`. This process mimics the lookup the you would perform in an actual neural network during translation tasks.\n",
    "\n",
    "This matrix representation is a precursor to understanding how more complex neural network architectures, like those using self-attention, manage token translations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
    "K = \n",
    "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
    "V =\n",
    "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token : tensor([0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating how to look up a translation for a given word using matrix operations\n",
    "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
    "q = \n",
    "# Display the query token vector\n",
    "print(\"Query token :\", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select key (K) : tensor([0., 0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select value (V): tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Use the index found from the key selection to find the corresponding value vector in V (output dictionary matrix)\n",
    "# This operation selects the row from V that is the translation of 'sous' in the output vocabulary\n",
    "print(\"Select value (V):\", )\n",
    "\n",
    "# The final output demonstrates how 'sous' can be translated using the neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code introduces a function for decoding one-hot vectors to tokens and updates the translation function to utilize matrix multiplication:\n",
    "\n",
    "### Decode one-hot vector\n",
    "The `decode_one_hot` function is designed to decode a one-hot encoded vector back into the corresponding token (word). It does this by finding the token whose one-hot representation has the highest cosine similarity with the given vector, which is effectively just the dot product due to the nature of one-hot vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_one_hot(one_hot, vector):\n",
    "\n",
    "\n",
    "    return   # Return the token corresponding to the one-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-based translate function\n",
    "The `translate` function now leverages matrix operations to perform the translation. For each token in the input sentence, it finds its one-hot vector, multiplies it with the matrices `K.T` and `V` to find the corresponding one-hot vector in the output vocabulary, and then decodes this vector to get the translated word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "\n",
    "\n",
    "    return   # Return the translated sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation test\n",
    "The improved translate function is tested with the sentence \"le chat est sous la table\", verifying that it correctly translates to \"the cat is under the table\" using the matrix operations for a seamless word-by-word translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This enhanced approach shows how neural network models can translate languages by representing the translation dictionary as matrices and using vector operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next code segment introduces concepts that lead up to the implementation of \"Attention\" in neural networks:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function for similarity\n",
    "It is explained that similar tokens will have similar vectors, and a softmax function is added to the equation. This function is applied to the output of the matrix multiplication of the query vector `q` and the transpose of the matrix `K`. The softmax function converts these values into probabilities, emphasizing the most similar token while still considering the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{table} =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print('E_{table} = ',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with attention mechanism\n",
    "The `translate` function is modified to use the softmax function as a way of applying attention. It first finds the one-hot vector for the token, then applies the softmax function to the dot product of `q` and `K.T`, scales it by the square root of the dimensionality (for normalization purposes), and finally multiplies this by `V` to get the output vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "\n",
    "    return   # Return the translated sentence\n",
    "\n",
    "# Test the translate function\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Translation**: The updated translate function is tested to ensure it correctly processes the sample sentence \"le chat est sous la table\", translating it to \"the cat is under the table\". This verifies that the attention mechanism implemented using softmax works as intended.\n",
    "\n",
    "This step marks the progression from simple look-up-based translation to an attention-based approach, introducing students to a key component of modern neural translation models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next part of the code demonstrates an improvement in the translation process by handling all queries in parallel:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the 'Q' matrix\n",
    "The matrix `Q` is constructed by stacking the one-hot encoded vectors of all tokens in the input sentence. This parallelizes the process of preparing the query vectors, which is more efficient than doing it sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# The sentence we want to translate\n",
    "sentence = \"le chat est sous la table\"\n",
    "\n",
    "# Stack all the one-hot encoded vectors for the tokens in the sentence to form the Q matrix\n",
    "Q = \n",
    "\n",
    "# Display the Q matrix\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated translate function\n",
    "The `translate` function is revised to use matrix multiplication across the entire sentence. Instead of translating word by word, it now uses the \"Q\" matrix to perform the operation in parallel for all words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "\n",
    "    return \n",
    "    \n",
    "# Test the function to ensure it produces the correct translation\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Efficiency improvement**: By applying operations to the entire sentence at once, this approach simulates a key aspect of the actual attention mechanism used in neural networks, which is processing multiple components of input data in parallel for faster computation.\n",
    "\n",
    "- **Test output**: The updated function correctly translates the French sentence \"le chat est sous la table\" to \"the cat is under the table\", confirming that the parallelization works effectively.\n",
    "\n",
    "This optimization hints at the computational advantages of matrix operations in neural networks, particularly for tasks like translation which benefit from parallel processing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "prev_pub_hash": "4def3ba5cde61eaebaab3454049b492158e60840dee0350f63f5db4f340ce9b1"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
