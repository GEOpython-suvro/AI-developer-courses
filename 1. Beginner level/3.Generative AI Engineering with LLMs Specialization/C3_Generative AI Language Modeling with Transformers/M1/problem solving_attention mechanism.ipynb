{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define 'vocabularies'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
      "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
     ]
    }
   ],
   "source": [
    "# Create and sort the input vocabulary from the dictionary's keys\n",
    "vocabulary_in = torch.sorted(list(set(dictionary.keys())))\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(vocabulary_in)\n",
    "\n",
    "# Create and sort the output vocabulary from the dictionary's values\n",
    "vocabulary_out = torch.sorted(list(set(dictionary.values())))\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(vocabulary_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode tokens using 'one hot' encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
    "def encode_one_hot(vocabulary):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return   # Return the dictionary of words and their one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
      "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
      "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
      "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
      "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
      "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
    "one_hot_in = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
      "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
      "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
      "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
      "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
      "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
    "# This visualizes the one-hot representation for each word in the input vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\t: tensor([1., 0., 0., 0., 0.])\n",
      "is\t: tensor([0., 1., 0., 0., 0.])\n",
      "table\t: tensor([0., 0., 1., 0., 0.])\n",
      "the\t: tensor([0., 0., 0., 1., 0.])\n",
      "under\t: tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
    "# This time we're encoding the target language vocabulary\n",
    "one_hot_out = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
    "K = \n",
    "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
    "V =\n",
    "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token : tensor([0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating how to look up a translation for a given word using matrix operations\n",
    "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
    "q = \n",
    "# Display the query token vector\n",
    "print(\"Query token :\", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select value (V): tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Use the index found from the key selection to find the corresponding value vector in V (output dictionary matrix)\n",
    "# This operation selects the row from V that is the translation of 'sous' in the output vocabulary\n",
    "print(\"Select value (V):\", )\n",
    "\n",
    "# The final output demonstrates how 'sous' can be translated using the neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code introduces a function for decoding one-hot vectors to tokens and updates the translation function to utilize matrix multiplication:\n",
    "\n",
    "### Decode one-hot vector\n",
    "The `decode_one_hot` function is designed to decode a one-hot encoded vector back into the corresponding token (word). It does this by finding the token whose one-hot representation has the highest cosine similarity with the given vector, which is effectively just the dot product due to the nature of one-hot vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def decode_one_hot(one_hot, vector):\n",
    "    best_sim=0\n",
    "    best_key=None\n",
    "    for k,v in one_hot.items():\n",
    "        cosine_sim=torch.dot(vector,v)\n",
    "        if cosine_sim>best_sim:\n",
    "            best_sim=cosine_sim\n",
    "            best_key=k\n",
    "    return best_key  # Return the token corresponding to the one-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-based translate function\n",
    "The `translate` function now leverages matrix operations to perform the translation. For each token in the input sentence, it finds its one-hot vector, multiplies it with the matrices `K.T` and `V` to find the corresponding one-hot vector in the output vocabulary, and then decodes this vector to get the translated word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sentence_out=\" \"\n",
    "    for token_in in tokenize(sentence):\n",
    "        q=encode_one_hot[token_in]\n",
    "        out= q @ K.T @ V\n",
    "        token_out=decode_one_hot(one_hot_out,out)\n",
    "        sentence_out+=token_out+\" \"\n",
    "    return sentence_out.strip()   # Return the translated sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation test\n",
    "The improved translate function is tested with the sentence \"le chat est sous la table\", verifying that it correctly translates to \"the cat is under the table\" using the matrix operations for a seamless word-by-word translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This enhanced approach shows how neural network models can translate languages by representing the translation dictionary as matrices and using vector operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next code segment introduces concepts that lead up to the implementation of \"Attention\" in neural networks:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function for similarity\n",
    "It is explained that similar tokens will have similar vectors, and a softmax function is added to the equation. This function is applied to the output of the matrix multiplication of the query vector `q` and the transpose of the matrix `K`. The softmax function converts these values into probabilities, emphasizing the most similar token while still considering the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{table} =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print('E_{table} = ',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with attention mechanism\n",
    "The `translate` function is modified to use the softmax function as a way of applying attention. It first finds the one-hot vector for the token, then applies the softmax function to the dot product of `q` and `K.T`, scales it by the square root of the dimensionality (for normalization purposes), and finally multiplies this by `V` to get the output vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    sentence_out=\" \"\n",
    "    for token_in in tokenize(sentence):\n",
    "        q=encode_one_hot[token_in]\n",
    "        out= torch.softmax(q @ K.T,dim=0) @ V\n",
    "        token_out=decode_one_hot(one_hot_out,out)\n",
    "        sentence_out+=token_out+\" \"\n",
    "    return sentence_out.strip()   # Return the translated sentence\n",
    "\n",
    "# Test the translate function\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Translation**: The updated translate function is tested to ensure it correctly processes the sample sentence \"le chat est sous la table\", translating it to \"the cat is under the table\". This verifies that the attention mechanism implemented using softmax works as intended.\n",
    "\n",
    "This step marks the progression from simple look-up-based translation to an attention-based approach, introducing students to a key component of modern neural translation models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next part of the code demonstrates an improvement in the translation process by handling all queries in parallel:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the 'Q' matrix\n",
    "The matrix `Q` is constructed by stacking the one-hot encoded vectors of all tokens in the input sentence. This parallelizes the process of preparing the query vectors, which is more efficient than doing it sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# The sentence we want to translate\n",
    "sentence = \"le chat est sous la table\"\n",
    "\n",
    "# Stack all the one-hot encoded vectors for the tokens in the sentence to form the Q matrix\n",
    "Q = \n",
    "\n",
    "# Display the Q matrix\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated translate function\n",
    "The `translate` function is revised to use matrix multiplication across the entire sentence. Instead of translating word by word, it now uses the \"Q\" matrix to perform the operation in parallel for all words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    q=torch.stack( encode_one_hot[k] for k in tokenize(sentence))\n",
    "    out = torch.softmax(q @ K.T)@ V\n",
    "    return \" \".join(decode_one_hot(one_hot_out,o)for o in out)\n",
    "    \n",
    "# Test the function to ensure it produces the correct translation\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Efficiency improvement**: By applying operations to the entire sentence at once, this approach simulates a key aspect of the actual attention mechanism used in neural networks, which is processing multiple components of input data in parallel for faster computation.\n",
    "\n",
    "- **Test output**: The updated function correctly translates the French sentence \"le chat est sous la table\" to \"the cat is under the table\", confirming that the parallelization works effectively.\n",
    "\n",
    "This optimization hints at the computational advantages of matrix operations in neural networks, particularly for tasks like translation which benefit from parallel processing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "prev_pub_hash": "4def3ba5cde61eaebaab3454049b492158e60840dee0350f63f5db4f340ce9b1"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
