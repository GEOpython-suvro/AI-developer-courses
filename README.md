# LLM Prompting best practices:
-**Be specific:** Provide detail and context about your problem \
-**Assign a role:** Assign a role to tailor the output you receive \
-**Request an expert opinion:** Assign an expert role and ask the LLM to evaluate the work you've already done to further refine it \
-**Give feedback:** Iteratively prompt the LLM and provide feedback on the output you receive to get closer to your expected results 

## Papers to code from scratch
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
- [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486)
- [RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs](https://arxiv.org/pdf/2404.08555)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401)
- [AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)
- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)

## Papers to read to get advantages in Language MODELS
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)

## Learn PyTorch for Deep Learning: Zero to Mastery book
- [Pytorch 0 to mastery](https://www.learnpytorch.io/)