## Papers to code from scratch
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)-2017
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)-Dec 2014
- [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473)-May 2016
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401)-April 2021
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)-July 2020
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)-Sep 2023
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)-Sep 2023
- [AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)-June 2021
- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)-Oct 2021
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)-June 2022
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)-Feb 2023
- [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)-May 2023

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)-May 2019
- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)-Feb 2003
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)-Sep 2013

## Learn PyTorch for Deep Learning: Zero to Mastery book
- [Pytorch 0 to mastery](https://www.learnpytorch.io/)