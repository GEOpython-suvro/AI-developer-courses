# LLM Prompting best practices:
-**Be specific:** Provide detail and context about your problem \
-**Assign a role:** Assign a role to tailor the output you receive \
-**Request an expert opinion:** Assign an expert role and ask the LLM to evaluate the work you've already done to further refine it \
-**Give feedback:** Iteratively prompt the LLM and provide feedback on the output you receive to get closer to your expected results 

## Papers to code from scratch
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)
- [AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401)
- [Handwritten Digit Recognition with a Back-Propagation Network ](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf)
- [Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325)

## Papers to read to get advantages in Language MODELS
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)

## Learn PyTorch for Deep Learning: Zero to Mastery book
- [Pytorch 0 to mastery](https://www.learnpytorch.io/)